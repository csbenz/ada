{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eat the season\n",
    "get some food per season from [eat the season](http://www.eattheseasons.com/seasons.php) website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>January</td>\n",
       "      <td>broccoli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>January</td>\n",
       "      <td>broccolini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>January</td>\n",
       "      <td>brussels sprouts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>January</td>\n",
       "      <td>butternut squash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>January</td>\n",
       "      <td>celery root</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month              food\n",
       "0  January          broccoli\n",
       "1  January        broccolini\n",
       "2  January  brussels sprouts\n",
       "3  January  butternut squash\n",
       "4  January       celery root"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eat_season_data = pd.DataFrame(columns=['month', 'food'])\n",
    "\n",
    "for month in months:\n",
    "    r = requests.get('http://www.eattheseasons.com/{0}.php'.format(month))\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    food_on_month = soup.find_all('p')\n",
    "    for p in food_on_month:\n",
    "        for elem in p.text.split(\", \"):\n",
    "            if (elem.lower() != month.lower()):\n",
    "                eat_season_data = eat_season_data.append({\n",
    "                    'month': month,\n",
    "                    'food': elem.strip(),\n",
    "                }, ignore_index=True)\n",
    "\n",
    "eat_season_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seasonal food guide\n",
    "Get some food per season from [seasonal food guide](https://www.seasonalfoodguide.org/) website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSeasonalFoodGuideCSV():\n",
    "    \n",
    "    url = 'https://www.seasonalfoodguide.org'\n",
    "    \n",
    "    bimonthly = []\n",
    "    for month in months:\n",
    "        bimonthly.append('early-{0}'.format(month.lower()))\n",
    "        bimonthly.append('late-{0}'.format(month.lower()))\n",
    "\n",
    "    #All the data on pages are generated by a script, let's obtain the adress\n",
    "    r = requests.get('https://www.seasonalfoodguide.org/maine/late-january')\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    url_end = soup.find_all('script')[1]['src']\n",
    "    req = requests.get(url + url_end)\n",
    "    data = req.text\n",
    "\n",
    "    season_guid_data = pd.DataFrame(columns=['month', 'food', 'state'])\n",
    "\n",
    "    m = re.findall(r'\\{name:\"(.*?)\\}\\}',data)\n",
    "    m = m[1:]\n",
    "\n",
    "    for elem in m:\n",
    "        #get each month per states from the current eatable\n",
    "        seasons = re.findall(r'[A-Z]{2}:{seasons:\\[(.*?)\\]', elem)\n",
    "        #get each states that has some season on the current eatable\n",
    "        states = re.findall(r'([A-Z]{2}):', elem)[1:]\n",
    "        #get the name of the current eatable\n",
    "        food = re.findall(r'([A-Za-z]+)\"', elem)[0]\n",
    "        for x in range(0, len(seasons)):\n",
    "            for season in seasons[x].split(','):\n",
    "                season_guid_data = season_guid_data.append({\n",
    "                        'month' : bimonthly[int(season)-1],\n",
    "                        'food' : food,\n",
    "                        'state' : states[x]\n",
    "            \n",
    "                }, ignore_index=True)\n",
    "    return season_guid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a csv dataset\n",
    "As the seasonal food guide is pretty big with the states, using regular expression in such a big file is long. We create a csv and then read it to compute the notebook faster. \n",
    "\n",
    "Regarding the space of our project, we put all the data folder into the .gitignore. Thus the first time the user compute the project, it as to generate again all the csv. This idea will be kept in all other CSV generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>month</th>\n",
       "      <th>food</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>late-july</td>\n",
       "      <td>Apples</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>early-august</td>\n",
       "      <td>Apples</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>late-august</td>\n",
       "      <td>Apples</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>early-september</td>\n",
       "      <td>Apples</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>late-september</td>\n",
       "      <td>Apples</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            month    food state\n",
       "0           0        late-july  Apples    AL\n",
       "1           1     early-august  Apples    AL\n",
       "2           2      late-august  Apples    AL\n",
       "3           3  early-september  Apples    AL\n",
       "4           4   late-september  Apples    AL"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "my_file = Path(\"data/seasonalFoodGuide.csv\")\n",
    "if my_file.is_file():\n",
    "    season_guid_data = pd.read_csv('data/seasonalFoodGuide.csv')\n",
    "else:\n",
    "    season_guid_data = buildSeasonalFoodGuideCSV()\n",
    "    season_guid_data.to_csv('data/seasonalFoodGuide.csv')\n",
    "            \n",
    "#TODO: Do we need to save this dataset as a csv as it took some times to generate?\n",
    "season_guid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyse our recipies dataset\n",
    "Our dataset contains 2,5GB of html file (110'517 file regarding the number of line in the log file). There is plenty of different results. We first need to analyse what kind of data we have before analyse the data themself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The log file\n",
    "We first have a log file containing valuable information. It give us the name of each file associated to the url it come from. We use it as index for the reste of the project. We will use the log file to navigate instead of looking blind in each file.\n",
    "**We saw in the logfile that some file came with some error. We keep that in mind and will come back later on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_path = 'data/recipePages/msg.log'\n",
    "recipies_path = 'data/recipePages'\n",
    "\n",
    "f = open(log_path,'r')\n",
    "log = f.read().split('\\n')\n",
    "#TODO: Take care of the error line in the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-23b0e79ffb08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[1;34m'url'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                         \u001b[1;34m'file'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                     }, ignore_index=True)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mlog_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def buildLogData():\n",
    "    log_data = pd.DataFrame(columns=['domain', 'url', 'file'])\n",
    "    for line in log:\n",
    "        domain = re.search(r'http://(.*?)/', line)\n",
    "        url = re.search(r'http://.*?(?=\\t)', line)\n",
    "        file_name = re.search(r'.*?(?=\\t)', line)\n",
    "        if domain is not None:\n",
    "            if url is not None:\n",
    "                if file_name is not None:\n",
    "                    log_data = log_data.append({\n",
    "                            'domain' : domain.group(0),\n",
    "                            'url' : url.group(0),\n",
    "                            'file' : file_name.group(0),\n",
    "                        }, ignore_index=True)\n",
    "    return log_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_file = Path(\"data/recipePages/log_data.csv\")\n",
    "if my_file.is_file():\n",
    "    log_data = pd.read_csv('data/recipePages/log_data.csv')\n",
    "else:\n",
    "    log_data = buildSeasonalFoodGuideCSV()\n",
    "    log_data.to_csv('data/recipePages/log_data.csv')\n",
    "            \n",
    "#TODO: Do we need to save this dataset as a csv as it took some times to generate?\n",
    "log_data.head()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the domain name's propotion?\n",
    "As we have many different domain name, we want to know how many of each domain name we have. To see then what to do with these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = log_data['domain'].value_counts()\n",
    "print('size: {0}'.format(df.size))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- allrecipes.com\n",
    "- www.food.com\n",
    "- www.foodnetwork.com\n",
    "\n",
    "These 3 website cover 49.77% of our dataset. We will then first make some methode to extract data from these and if needed or if we have time, we will then take care of the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the ingredients of each recipes\n",
    "We have a second dataset containing the recipe name, url, domain, ingredients and many other information. We will then extract this dataset and merge it to our log_data datafram to have the possibility to link these information to the corresponding html file. We do this because the review information we are only availible in the html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recip_info_path = 'data/recipeInfo/recipeInfo_WestWhiteHorvitz_WWW2013.tsv'\n",
    "\n",
    "recip_info = pd.read_csv(recip_info_path, sep='\\t', encoding='latin-1')\n",
    "\n",
    "restricted_recipe_info = recip_info[['url', 'title', 'ingredients_list']]\n",
    "\n",
    "merged_info = pd.merge(log_data, restricted_recipe_info, how='inner', on='url', indicator=False, suffixes=('_info', '_log'))\n",
    "merged_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the dataset\n",
    "Currently, our dataset is hudge. Going throug it completly took some time. To simplify our research acording to our previous observation, we will only keep the 3 main domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keeped_domain = pd.DataFrame({'domain':['http://allrecipes.com/', 'http://www.food.com/', 'http://www.foodnetwork.com/']})\n",
    "\n",
    "new_merged = merged_info[merged_info.domain.isin(keeped_domain.domain)].reset_index()\n",
    "new_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Review date\n",
    "We do **the assumption** that the people which do a recipe will review the same day or maximum in the same week he cook the recipe. We need to extract the date of all reviews to know when they cooked the recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### allrecipes.com\n",
    "Starting for allrecipes.com. An inspection on the html elements lead us to see that all review date are referenced in:\n",
    "\n",
    "``<div class=\"review\">``\n",
    "\n",
    "To find it, we used the inspector feature in firefox. It apears on testing that many html file we have are malformed. Sometime a page is just a search on a food name and it's not a recipe. Some othertime, there is no review. We had to modify the following methods manytime to take these error into account.\n",
    "As we first parcour the entire dataset to extracte the useful information, we decide to put some nul value when the data are malformed. We will also have to take care of the quantity associated to the ingredients name. But we keep it for later.\n",
    "\n",
    "We also had the surprise that BeautifulSoup search by matching element. It lead to the following problem, searching class review give us all class containging the word 'review' like 'previre' and many others. To deal with this problem and only get our class, we modify our usual way to search with BeautifulSoup and use an anonymus function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allRecipesReviewDate(path):\n",
    "    f = open(path, 'r', encoding='latin-1')\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    #The mentionned lamnda function for BeautifulSoup search\n",
    "    review_html = soup.find_all(lambda tag: tag.name =='div' and tag.get('class') == ['review'])\n",
    "    reviews = ''\n",
    "    for rev in review_html:\n",
    "        if rev is not None:\n",
    "            if reviews != '':\n",
    "                reviews += ' - '\n",
    "            text = rev.text.strip().replace('\\n', '').replace('\\t', '')\n",
    "            reviews += re.search(r'[A-Z][a-z]{2}\\. [0-9]*, 200[0-9]', text).group(0)\n",
    "    return reviews\n",
    "\n",
    "#Example of result with a random file on this domain\n",
    "allRecipesReviewDate('data/recipePages/7e0ad7374f08c4a8de3500c065c17180.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### www.food.com\n",
    "Same principe, we use the inspector on firefox to indentify the review date. This time, there is no class easely findable directly for the date. We goes up to the first one acceptable and the  do a second find_all on it. As there is two ``<p>`` elements this time and we are interessting in the second one, we just take only the second element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foodReviewDate(path):\n",
    "    f = open(path, 'r', encoding='latin-1')\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    review_html = soup.find_all('div', class_=\"about-recipe-info\")\n",
    "    reviews = ''\n",
    "    for rev in review_html:\n",
    "        if rev is not None:\n",
    "            if reviews != '':\n",
    "                reviews += ' - '\n",
    "            reviews += rev.find_all('p')[1].text    \n",
    "    return reviews\n",
    "foodReviewDate('data/recipePages/60e9148725c3f64336fc9d83b2c1b521.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### www.foodnetwork.com\n",
    "Same procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foodnetworkReviewDate(path):\n",
    "    f = open(path, 'r', encoding='latin-1')\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    review_html = soup.find_all('div', class_=\"about-recipe-info\")\n",
    "    reviews = ''\n",
    "    for rev in review_html:\n",
    "        if rev is not None:\n",
    "            if reviews != '':\n",
    "                reviews += ' - '\n",
    "            reviews += rev.find_all('p')[2].text    \n",
    "    return reviews\n",
    "foodnetworkReviewDate('data/recipePages/10cf272724e823b8038b8190addf04d3.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the review date of each recipes\n",
    "Now we can get a all the review date of a html file on our 3 favorite website, let's create a table with all of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Reset the index in the new_merged dataframe\n",
    "\n",
    "html_file_path = 'data/recipePages/'\n",
    "reviews = pd.DataFrame(columns=['reviews_dates' : text,])\n",
    "index = 0\n",
    "for line in new_merged['domain']:    \n",
    "    text = 'NaN'\n",
    "    if line == 'http://allrecipes.com/':\n",
    "        text = allRecipesReviewDate(html_file_path + new_merged['file'][index])\n",
    "    if line == 'http://www.food.com/':\n",
    "        text = foodReviewDate(html_file_path + new_merged['file'][index])\n",
    "    if line == 'http://www.foodnetwork.com/':\n",
    "        text = foodnetworkReviewDate(html_file_path + new_merged['file'][index])\n",
    "    reviews = reviews.append({\n",
    "        'reviews_dates' : text,\n",
    "    }, ignore_index=True)\n",
    "    index += 1\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html_file_path = 'data/recipePages/'\n",
    "recipes_ingredient = pd.DataFrame(columns=['recipe_name', 'domain', 'file', 'ingredients'])\n",
    "index = 0\n",
    "for line in log_data['domain']:  \n",
    "    #print(index)\n",
    "    if line == 'http://allrecipes.com/' or line == 'http://www.food.com/' or line == 'http://www.foodnetwork.com/':\n",
    "        ingredients = []\n",
    "        title = ''\n",
    "        if line == 'http://allrecipes.com/':\n",
    "            ingredients, title = allRecipesIngredients(html_file_path + log_data['file'][index])\n",
    "        if line == 'http://www.food.com/':\n",
    "            ingredients, title = foodIngredients(html_file_path + log_data['file'][index])\n",
    "        if line == 'http://www.foodnetwork.com/':\n",
    "            ingredients, title = foodNetworkIngredients(html_file_path + log_data['file'][index])\n",
    "        recipes_ingredient = recipes_ingredient.append({\n",
    "                'recipe_name' : title,\n",
    "                'domain' : line,\n",
    "                'file' : log_data['file'][index],\n",
    "                'ingredients' : ingredients\n",
    "            }, ignore_index=True)\n",
    "    index += 1\n",
    "    \n",
    "#TODO: Save this as a csv as it take age to compute. So we don't have to compute it again all the time.\n",
    "recipes_ingredient.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the date of recipe's review with season of its ingredients\n",
    "As our goal is to figure out if the foods that grow during precise natural seasons are actually eaten during that time. We have to compare the review date with the season information we got from other website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO compare the date with our seasonal information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map of the USA's States\n",
    "Having a visual representation of our work is really helpfull for basic validation on our part. It is also better to explain what we did with example. Curently it's just the USA's States, we will implement it when we will have data to inject in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_geojson_path = os.path.join('GeoJson', 'gz_2010_us_states_500k.json')\n",
    "usa_geojson = json.load(open(usa_geojson_path))\n",
    "\n",
    "usa_map = folium.Map(location=[48, -102], zoom_start=3)\n",
    "\n",
    "usa_states = []\n",
    "for i in usa_geojson['features']:\n",
    "    usa_states.append(i['properties']['NAME'])\n",
    "\n",
    "folium.GeoJson(usa_geojson).add_to(usa_map)\n",
    "\n",
    "#TODO: Inject usefull data in it.\n",
    "usa_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final visualisation\n",
    "We have many data, many relation. It's time to give a life to all these information! The last part will be to represente these relation as concret as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "# Archive\n",
    "These cells are some elements we had but which took too many time to run or are code we simplify or don't use anymore.\n",
    "We don't want to delet it as we took time to write them and as we can re-use a part of them. We will move these away for the delivery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### allrecipes.com\n",
    "Starting for allrecipes.com. An inspection on the html elements lead us to see that all ingredient are referenced in:\n",
    "\n",
    "```<li class=\"plaincharacterwrap ingredient\">text</li>```\n",
    "\n",
    "To find it, we used the inspector feature in firefox. It apears on testing that many html file we have are malformed. Sometime a page is just a search on a food name and it's not a recipe. Some othertime, the recipe is not finish, and then the ingredient list contain some blanks. We had to modify the following methods manytime to take these error into account.\n",
    "As we first parcour the entire dataset to extracte the useful information, we decide to put some nul value when the data are malformed. We will also have to take care of the quantity associated to the ingredients name. But we keep it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def allRecipesIngredients(path):\n",
    "    f = open(path, 'r', encoding='latin-1')\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    ingredients_html = soup.find_all('li', class_=\"plaincharacterwrap ingredient\")\n",
    "    title_html = soup.find_all('h1', class_='plaincharacterwrap fn')\n",
    "    #The data on allrecipes sometime are not recipes but just a search on a word\n",
    "    title = ''\n",
    "    if len(title_html) > 0:\n",
    "        title = title_html[0].find('span', class_='itemreviewed').text\n",
    "    ingredients = ''\n",
    "    for ingr in ingredients_html:\n",
    "        if ingr is not None:\n",
    "            if ingredients != '':\n",
    "                ingredients += ', '\n",
    "            ingredients += ingr.text.strip()\n",
    "    return ingredients, title\n",
    "#Example of result with a random file on this domain\n",
    "results, title= allRecipesIngredients('data/recipePages/000a3333ad24828769b6be5a5e1bdb4a.html')\n",
    "\n",
    "#TODO: Format the data to only have the name of the food\n",
    "print(title)\n",
    "results'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### www.food.com\n",
    "Same principe, we use the inspector on firefox to indentify the ingredient. But this time we had a 'span' with the name value. So we don't have the quantity to take care now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def foodIngredients(path):\n",
    "    f = open(path, 'r', encoding='latin-1')\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    ingredients_html = soup.find_all('li', class_=\"ingredient\")\n",
    "    title_html = soup.find_all('h1', class_='fn')\n",
    "    title = ''\n",
    "    if len(title_html) > 0:\n",
    "        title = title_html[0].text\n",
    "    ingredients = ''\n",
    "    for ingr in ingredients_html:\n",
    "        if ingr.find('span', class_='name') is not None:\n",
    "            if ingredients != '':\n",
    "                ingredients += ', '        \n",
    "            ingredients += ingr.find('span', class_='name').text.strip().replace('\\n', '').replace('\\t', '')\n",
    "    return ingredients, title\n",
    "\n",
    "#Example of result with a random file on this domain\n",
    "results, title = foodIngredients('data/recipePages/60e9148725c3f64336fc9d83b2c1b521.html')\n",
    "print(title)\n",
    "results'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### www.foodnetwork.com\n",
    "Same as allrecipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def foodNetworkIngredients(path):\n",
    "    f = open(path, 'r', encoding='latin-1')\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    ingredients_html = soup.find_all('li', class_=\"ingredient\")\n",
    "    title_html = soup.find_all('h1', class_= 'fn')\n",
    "    title = ''\n",
    "    if len(title_html) > 0:\n",
    "        title = title_html[0].text\n",
    "    ingredients = ''\n",
    "    for ingr in ingredients_html:\n",
    "        if ingr is not None:\n",
    "            if ingredients != '':\n",
    "                ingredients += ', '\n",
    "            ingredients += ingr.text.strip()\n",
    "    return ingredients, title\n",
    "\n",
    "#Example of result with a random file on this domain\n",
    "results , title = foodNetworkIngredients('data/recipePages/10cf272724e823b8038b8190addf04d3.html')\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Format the data to only have the name of the food\n",
    "print(title)\n",
    "results'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the ingredients of each recipes\n",
    "Now we can get a all the ingredients of a html file on our 3 favorite website, let's create a table with all of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''html_file_path = 'data/recipePages/'\n",
    "recipes_ingredient = pd.DataFrame(columns=['recipe_name', 'domain', 'file', 'ingredients'])\n",
    "index = 0\n",
    "for line in log_data['domain']:  \n",
    "    #print(index)\n",
    "    if line == 'http://allrecipes.com/' or line == 'http://www.food.com/' or line == 'http://www.foodnetwork.com/':\n",
    "        ingredients = []\n",
    "        title = ''\n",
    "        if line == 'http://allrecipes.com/':\n",
    "            ingredients, title = allRecipesIngredients(html_file_path + log_data['file'][index])\n",
    "        if line == 'http://www.food.com/':\n",
    "            ingredients, title = foodIngredients(html_file_path + log_data['file'][index])\n",
    "        if line == 'http://www.foodnetwork.com/':\n",
    "            ingredients, title = foodNetworkIngredients(html_file_path + log_data['file'][index])\n",
    "        recipes_ingredient = recipes_ingredient.append({\n",
    "                'recipe_name' : title,\n",
    "                'domain' : line,\n",
    "                'file' : log_data['file'][index],\n",
    "                'ingredients' : ingredients\n",
    "            }, ignore_index=True)\n",
    "    index += 1\n",
    "    \n",
    "#TODO: Save this as a csv as it take age to compute. So we don't have to compute it again all the time.\n",
    "recipes_ingredient.head()'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
